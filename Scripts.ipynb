{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5991ec09",
   "metadata": {},
   "source": [
    "# Preprocessing Model's responses (Removing Duplication, noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed67122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "path = [] # include path to RAW files \n",
    "\n",
    "####################################\n",
    "# These are Regex patterns that I used for cleaning the RAW responses. \n",
    "def truncate_string(s):\n",
    "    return re.split(r'@@\\s*(Response|Test|Exception|Instance|Input|Additional Response|Reference|HumanEval Answer|Postprocess|Grading|Followup|Ground Truth|Human Response|Prompt|Example|Error|Alternate Response|Original|CoreDump|Solution|Explanation|Comment|Answer|Challenge|Expected Output|Output|Test Cases|Expected|Actual Output|Hint|Instruction|@|@@|@@@)', s, maxsplit=1)[0]\n",
    "\n",
    "def remove_icl_examples(text):\n",
    "    pattern = r\"(?:\\[Example 1\\]|public).*?\\[Your Turn\\] Buggy Code:\\s*\\n\"\n",
    "    return re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "\n",
    "def remove_icl_examples2(text):\n",
    "    pattern = r\".*?# Fixed Function:\\s*\\n\"\n",
    "    return re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "\n",
    "def truncate_string2(s):\n",
    "    return re.split(r'\\n\\n\\n\\s*(\\n/\\*|\\n\\*/|import|# Provide|Response|Original|Solution|Explanation|Comment|Answer|Challenge|Expected Output|Output|Test Cases|Expected|Actual Output|Hint|Instruction|@|@@|@@@)', s, maxsplit=1)[0]\n",
    "\n",
    "def clean_string(s):\n",
    "    # Remove @@ sequences only if they appear after two newlines\n",
    "    return re.sub(r'(\\S+)\\s*\\n\\n\\s*(@+\\s*)+', r'\\1', s).strip()\n",
    "\n",
    "def remove_after_second_occurrence(text, phrase=\"package humaneval.buggy;\"):\n",
    "    parts = text.split(phrase, 2)  \n",
    "    if len(parts) > 2:\n",
    "        return phrase.join(parts[:2]) \n",
    "    return text  \n",
    "####################################\n",
    "\n",
    "\n",
    "# For each RAW files included in path, clean the responses and output the CLEAN file\n",
    "for bench in path:\n",
    "    with open(f\"{bench}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for benchmark in data['data']:\n",
    "        for i in range(len(data['data'][benchmark]['output'])):\n",
    "            text = data['data'][benchmark]['output'][i]\n",
    "            data['data'][benchmark]['output'][i] = clean_string((remove_after_second_occurrence(truncate_string(text.replace(\"Write a solution to the following coding problem:\\nThe input is buggy code, you are given the logic of the function in the comment block. Base on that and fix the functionality to match the logic.\",\"\")))))\n",
    "            \n",
    "    # Save the test file\n",
    "    with open(f\"{bench}_clean.json\", \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7dab2",
   "metadata": {},
   "source": [
    "# Running unit tests for cleaned resposnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee302f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "path = [] # include path to CLEAN files \n",
    "\n",
    "benchmark_dir = \"\"\n",
    "test_results = {}\n",
    "\n",
    "# For reponses started with \"public class\"\n",
    "def replace_public_class(java_file, new_class_code, pattern):\n",
    "    with open(java_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    imports = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"import \"):  \n",
    "            imports.append(line)\n",
    "        elif line.strip().startswith(\"package \"):\n",
    "            imports.append(line)\n",
    "    \n",
    "    new_code = \"\".join(imports) + \"\\n\" + new_class_code\n",
    "\n",
    "    with open(java_file, \"w\") as f:\n",
    "        f.write(new_code)\n",
    "\n",
    "# For reponses started with \"public static\"\n",
    "def replace_public_static(java_file, new_class_code, pattern):\n",
    "    with open(java_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    imports = []\n",
    "    in_class = False\n",
    "    in_feature = False\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"import \"): \n",
    "            imports.append(line)\n",
    "        elif line.strip().startswith(\"package \"):\n",
    "            imports.append(line)\n",
    "        elif line.strip().startswith(pattern.split(\"{\")[0]):\n",
    "            in_feature = True\n",
    "        elif (line.strip().startswith(\"public class\") or in_class) and (not in_feature):  \n",
    "            in_class = True\n",
    "            imports.append(line)\n",
    "        \n",
    "    \n",
    "    new_code = \"\".join(imports) + \"\\n\" + new_class_code + \"\\n}\"\n",
    "\n",
    "    with open(java_file, \"w\") as f:\n",
    "        f.write(new_code)\n",
    "\n",
    "# For reponses started with \"package\"\n",
    "def replace_class_body(java_file, new_class_code, pattern):\n",
    "    with open(java_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(java_file, \"w\") as f:\n",
    "        f.write(new_class_code)\n",
    "\n",
    "# Will run junit-tests and create a result file\n",
    "def test_response(project_name):\n",
    "    try:\n",
    "        result = subprocess.run([\"mvn\", \"test\", f\"-Dtest=TEST_{project_name}.java\"], \n",
    "                                capture_output=True, text=True, timeout=10)\n",
    "\n",
    "        output = result.stdout + result.stderr\n",
    "        output_upper = output.upper()\n",
    "        \n",
    "        if \"BUILD FAILURE\" in output_upper:\n",
    "            if \"FAILURES:\" in output_upper:\n",
    "                return \"wrong\"\n",
    "            return \"uncompilable\"\n",
    "        elif \"TIMEOUT\" in output_upper:\n",
    "            return \"timeout\"\n",
    "        elif \"BUILD SUCCESS\" in output_upper:\n",
    "            return \"plausible\"\n",
    "        else:\n",
    "            return \"wrong\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running tests for {project_name}: {e}\")\n",
    "        return \"uncompilable\"\n",
    "\n",
    "# This will go into the CLEAN response files, run junit-test on each resposnes through getting each output, replace it into the Buggy Java class, then run JUnit test\n",
    "for bench in path:\n",
    "    with open(f\"{bench}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for project_name, details in data['data'].items():\n",
    "        print(project_name)\n",
    "        buggy_file = f\"src/main/java/humaneval/buggy/{project_name}.java\"\n",
    "        \n",
    "        with open(buggy_file, \"r\") as f:\n",
    "            buggy_code = f.readlines()\n",
    "\n",
    "        test_results[project_name] = {}\n",
    "        pattern = details['input']\n",
    "        for rank, patch in enumerate(details[\"output\"]):\n",
    "\n",
    "            if str(patch).startswith(\"package\"):\n",
    "                replace_class_body(buggy_file, patch,pattern)\n",
    "\n",
    "            elif str(patch).startswith(\"public class\"):\n",
    "                replace_public_class(buggy_file, patch,pattern)\n",
    "\n",
    "            else:\n",
    "                replace_public_static(buggy_file, patch,pattern)\n",
    "            \n",
    "            correctness = test_response(project_name)\n",
    "\n",
    "            test_results[project_name][rank] = correctness\n",
    "            print(f\"Test result for {project_name} fix {rank}: {correctness}\")\n",
    "\n",
    "            # Replace with orignal code\n",
    "            with open(buggy_file, \"w\") as f: \n",
    "                f.writelines(buggy_code)\n",
    "\n",
    "    with open(f\"{bench}_results.json\", \"w\") as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"âœ… All tests completed. Results saved to test_results.json\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f9ce7",
   "metadata": {},
   "source": [
    "# For getting Pass@k results from RESULT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ce32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = [] # include path to RESULT files \n",
    "\n",
    "# This will go through every files included in path, get the pass@k results of those files\n",
    "for bench in path:\n",
    "    with open(f\"{bench}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    pass1 = 0\n",
    "    pass5 = 0\n",
    "    pass10 = 0\n",
    "    \n",
    "    print(bench)\n",
    "    for benchmark,num in data.items():\n",
    "        i = 1\n",
    "        done5 = False\n",
    "        done10 = False\n",
    "        for result in data[benchmark]:\n",
    "            if i == 1 and data[benchmark][result] == 'plausible':\n",
    "                pass1 = pass1 + 1\n",
    "            \n",
    "            if i <= 5 and data[benchmark][result] == 'plausible' and not done5:\n",
    "                pass5 = pass5 + 1\n",
    "                done5 = True\n",
    "\n",
    "            if i <= 10 and data[benchmark][result] == 'plausible' and not done10:\n",
    "                pass10 = pass10 + 1\n",
    "                done10 = True\n",
    "\n",
    "            i = i + 1\n",
    "    print(f\"Pass@1:{pass1}/163, Pass@5:{pass5}/163, Pass@10:{pass10}/163\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e0298",
   "metadata": {},
   "source": [
    "# Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a430082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig, PromptEncoderConfig, PrefixTuningConfig, IA3Config,\n",
    "    get_peft_model,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import sys\n",
    "from datasets import Dataset\n",
    "\n",
    "def main():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    model_name_or_path = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "    # Uncomment this for QLoRA\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\",quantization_config={\"load_in_4bit\": True, \"bnb_4bit_compute_dtype\": torch.bfloat16})\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Loading original dataset\n",
    "    dataset = load_dataset(\"zxliu/ReAPR-Automatic-Program-Repair-via-Retrieval-Augmented-Large-Language-Models\", split=\"train\")\n",
    "\n",
    "    # Random seed (so far this seed yields the best result)\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    # Select 30k dataset, with max token = 1250\n",
    "    def count_tokens(buggy_function, fixed_function):\n",
    "        buggy_tokens = tokenizer(buggy_function, return_tensors=\"pt\", truncation=False).input_ids.size(1)\n",
    "        fixed_tokens = tokenizer(fixed_function, return_tensors=\"pt\", truncation=False).input_ids.size(1)\n",
    "        return buggy_tokens + fixed_tokens\n",
    "    \n",
    "    selected_samples = []\n",
    "    max_samples = 30000\n",
    "\n",
    "    for sample in dataset:\n",
    "        if count_tokens(sample['buggy_function'], sample['fixed_function']) <= 1250:\n",
    "            selected_samples.append(sample)\n",
    "            if len(selected_samples) >= max_samples:\n",
    "                break  # Stop once we have 30,000 samples\n",
    "\n",
    "    data_dict = {\n",
    "        \"buggy_function\": [sample[\"buggy_function\"] for sample in selected_samples],\n",
    "        \"fixed_function\": [sample[\"fixed_function\"] for sample in selected_samples],\n",
    "    }\n",
    "\n",
    "    selected_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    # Instruction dataset, spliting 80 20 for training and evaluation\n",
    "    split_dataset = selected_dataset.train_test_split(train_size=24000, test_size=6000, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "    tokenizer.add_eos_token = True\n",
    "\n",
    "    if model_name_or_path == \"deepseek-ai/deepseek-coder-6.7b-base\":\n",
    "        tokenizer.pad_token_id = 32018 # this follow the previous study \n",
    "    else:\n",
    "        tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Tokenize the datasets for training\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=1350,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "                and len(result[\"input_ids\"]) < 1350\n",
    "                and add_eos_token\n",
    "            ):\n",
    "                result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "                result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Generate Prompt using standard prompting approach\n",
    "    def full_prompt_generation(data):\n",
    "        full_prompt =f\"\"\"You are an expert model in fixing program bugs. Your job is to deliver the most accurate fixes given a buggy program.\n",
    "\n",
    "        @@ Instruction:\n",
    "        {data[\"buggy_function\"]}\n",
    "\n",
    "        @@ Response:\n",
    "        {data[\"fixed_function\"]}\n",
    "    \"\"\"\n",
    "        full = tokenize(full_prompt)\n",
    "\n",
    "        question = tokenize(f\"\"\"You are an expert model in fixing program bugs. Your job is to deliver the most accurate fixes given a buggy program.\n",
    "\n",
    "        @@ Instruction:\n",
    "        {data[\"buggy_function\"]}\n",
    "\n",
    "        @@ Response:\n",
    "        \"\"\")\n",
    "\n",
    "        question_len = len(question['input_ids'])\n",
    "\n",
    "        # setting this helps model focus only on generating the fixed instead of reproducing input\n",
    "        full[\"labels\"] = [\n",
    "            -100\n",
    "        ] * question_len + full[\"labels\"][\n",
    "            question_len:\n",
    "        ]  \n",
    "        return full\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(full_prompt_generation, remove_columns=['buggy_function','fixed_function'])\n",
    "    tokenized_val_dataset = eval_dataset.map(full_prompt_generation, remove_columns=['buggy_function','fixed_function'])\n",
    "\n",
    "\n",
    "    model.train()  # put model back into training mode\n",
    "\n",
    "    # Uncomment this for QLoRA\n",
    "    # model = prepare_model_for_kbit_training(model) \n",
    "\n",
    "    # configs \n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # config = PromptEncoderConfig(\n",
    "    #         peft_type=\"P_TUNING\",\n",
    "    #         task_type=\"CAUSAL_LM\",\n",
    "    #         num_virtual_tokens=100,\n",
    "    #         encoder_hidden_size=2048,\n",
    "    #         encoder_reparameterization_type= \"MLP\"\n",
    "    # )\n",
    "\n",
    "    # config = IA3Config(\n",
    "    #         peft_type=\"IA3\",\n",
    "    #         task_type=\"CAUSAL_LM\",\n",
    "    #         )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    output_dir = \"/drive/MyDrive/my_model\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=3,\n",
    "            per_device_eval_batch_size=3,\n",
    "            gradient_accumulation_steps=1,\n",
    "            warmup_ratio=0.05,\n",
    "            num_train_epochs= 3,\n",
    "            learning_rate=1e-5,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            fp16=False,\n",
    "            bf16= True,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\", \n",
    "            save_strategy=\"no\",\n",
    "            eval_steps=0.2,\n",
    "            output_dir=output_dir,\n",
    "            load_best_model_at_end=False,\n",
    "            group_by_length=True, \n",
    "            report_to=\"none\", \n",
    "            run_name=None, \n",
    "            gradient_checkpointing=True,\n",
    "            dataloader_drop_last=True,\n",
    "            dataloader_pin_memory=True,\n",
    "            disable_tqdm = False,\n",
    "            dataloader_num_workers=4,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # for running in linux, speed up \n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        print(\"compiling the model\")\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d5d50",
   "metadata": {},
   "source": [
    "# Inference Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"codellama/CodeLlama-7b-hf\"\n",
    "tokenizer_name_or_path = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = 0 # unk. we want this to be different from the eos token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "ADAPTER_PATH = \"/drive/MyDrive/my_model_lora\"\n",
    "\n",
    "# For loading PEFT-trained models\n",
    "\n",
    "# QLORA\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\",quantization_config={\"load_in_4bit\": True, \"bnb_4bit_compute_dtype\": torch.float16})\n",
    "\n",
    "# LoRA, IA3, PTuning\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(ADAPTER_PATH,device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "\n",
    "# For loading Base models\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,device_map=device,torch_dtype=torch.bfloat16)\n",
    "\n",
    "# This class is used for generating Inference Prompt\n",
    "class generate_prompt:\n",
    "    PROMPT_TEMPLATE = \"\"\"You are an expert model in fixing program bugs. Your job is to deliver the most accurate fixes given a buggy program.\n",
    "\n",
    "@@ Instruction\n",
    "{instruction}\n",
    "\n",
    "@@ Response\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt(instruction: str) -> str:\n",
    "        return generate_prompt.PROMPT_TEMPLATE.format(instruction=instruction)\n",
    "\n",
    "# Accompany with class generate_prompt\n",
    "def generate_benchmark_prompt(text: str) -> str:\n",
    "    # This prompt will change depending on the Benchmark, below is for benchmark that includes additional examples (RQ2, RQ3)\n",
    "    BENCHMARK_PROMPT = \"\"\"Write a solution to the following coding problem:\n",
    "    The input is buggy code, you are given the logic of the function in the comment block. Base on that and fix the functionality to match the logic.\n",
    "    {problem}\"\"\"\n",
    "\n",
    "    # This is for benchmark with no context (RQ1)\n",
    "    # BENCHMARK_PROMPT = \"\"\"Write a solution to the following coding problem:\n",
    "    # The input is buggy code, fix it accordingly.\n",
    "    # {problem}\"\"\"\n",
    "\n",
    "    # This is for benchmark with Few-Shot settings (RQ2,RQ3)\n",
    "    # BENCHMARK_PROMPT = \"\"\"You are given some examples in the comment block on how to fix the target buggy code. Base on those examples, fix the last Buggy Code and only provide your response for the last Fixed Code section.\n",
    "    # {problem}\"\"\"\n",
    "\n",
    "    formatted_text = BENCHMARK_PROMPT.format(problem=text)\n",
    "    return generate_prompt.generate_prompt(instruction=formatted_text)\n",
    "\n",
    "output = json.load(open('3_examples_benchmark.json', 'r')) # Depending on which benchmark are being tested on\n",
    "output2 = json.load(open('0_examples_benchmark.json', 'r')) # This dataset acts as a limit for model response max length\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# This follow the same setting as previous study, except for max_length\n",
    "for benchmark in output['data']:\n",
    "    inputs = tokenizer(generate_benchmark_prompt(output['data'][benchmark]['input']), truncation=True,\n",
    "                    max_length=1800,\n",
    "                    padding=False,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "    \n",
    "    eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs['input_ids'].cuda(),\n",
    "        attention_mask=inputs['attention_mask'].cuda(),\n",
    "        max_new_tokens= tokenizer(output2['data'][benchmark]['input'], return_tensors=\"pt\", truncation=False).input_ids.size(1) + 256, # maximum expected length of response from models, this avoid too much noises\n",
    "        num_beams=10,\n",
    "        num_return_sequences=10,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=eos_id,\n",
    "    )\n",
    "\n",
    "    all_output = []\n",
    "    for generated_id in generated_ids:\n",
    "        text = tokenizer.decode(generated_id[len(inputs[0]):], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        all_output.append(text)\n",
    "\n",
    "    output['data'][benchmark]['output'] = all_output\n",
    "\n",
    "with open(f'codellama_qlora_3_icls.json', \"w\") as f:\n",
    "    json.dump(output, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
